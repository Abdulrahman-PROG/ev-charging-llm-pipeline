{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-1",
   "metadata": {},
   "source": [
    "# EV Charging LLM Fine-tuning Pipeline\n",
    "This notebook fine-tunes a small language model (â‰¤7B parameters) on electric vehicle charging domain data using LoRA/QLoRA techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"microsoft/DialoGPT-small\",  # Small model for demo\n",
    "    \"max_length\": 512,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"eval_batch_size\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"output_dir\": \"./fine_tuned_model\",\n",
    "    \"data_path\": \"output_data/ev_training_alpaca.json\",\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and directories\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Initialize experiment tracking (optional)\n",
    "experiment_name = f\"ev_charging_llm_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"Experiment name: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-data",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "with open(CONFIG[\"data_path\"], 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} training examples\")\n",
    "\n",
    "# Display first example\n",
    "if training_data:\n",
    "    print(\"\\nFirst training example:\")\n",
    "    print(f\"Instruction: {training_data[0]['instruction']}\")\n",
    "    print(f\"Input: {training_data[0]['input']}\")\n",
    "    print(f\"Output: {training_data[0]['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Format training examples into prompt-response format\"\"\"\n",
    "    if example['input']:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    response = example['output']\n",
    "    return prompt + response + \"</s>\"\n",
    "\n",
    "# Format all training examples\n",
    "formatted_data = [format_prompt(example) for example in training_data]\n",
    "\n",
    "print(f\"Formatted {len(formatted_data)} examples\")\n",
    "print(\"\\nFirst formatted example:\")\n",
    "print(formatted_data[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation\n",
    "train_texts, val_texts = train_test_split(\n",
    "    formatted_data, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_texts)}\")\n",
    "print(f\"Validation examples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-model",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded with {model.num_parameters():,} parameters\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]  # For DialoGPT\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA configuration applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-tokenize",
   "metadata": {},
   "source": [
    "## Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts})\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"Tokenized training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Tokenized validation dataset: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-training",
   "metadata": {},
   "source": [
    "## Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"eval_batch_size\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb for now\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-collator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"Data collator configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(f\"Training will run for {CONFIG['num_epochs']} epochs\")\n",
    "print(f\"Total training steps: {len(train_dataset) // CONFIG['train_batch_size'] * CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(f\"Start time: {datetime.now()}\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Training completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-save",
   "metadata": {},
   "source": [
    "## Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "final_model_path = os.path.join(CONFIG[\"output_dir\"], \"final_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_path = os.path.join(final_model_path, \"training_config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"Training configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-test",
   "metadata": {},
   "source": [
    "## Quick Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "def generate_response(prompt, max_length=200):\n",
    "    \"\"\"Generate response using the fine-tuned model\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test with EV charging questions\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nWhat are the benefits of fast charging for electric vehicles?\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nHow do I find charging stations near me?\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWhat is the difference between AC and DC charging?\\n\\n### Response:\\n\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt.split('### Response:')[0].split('### Instruction:')[1].strip()}\")\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-summary",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"=== Fine-tuning Summary ===\")\n",
    "print(f\"Base model: {CONFIG['model_name']}\")\n",
    "print(f\"Training examples: {len(train_texts)}\")\n",
    "print(f\"Validation examples: {len(val_texts)}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"LoRA rank: {CONFIG['lora_r']}\")\n",
    "print(f\"LoRA alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "# Get final training metrics\n",
    "if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "    final_metrics = trainer.state.log_history[-1]\n",
    "    print(\"\\nFinal metrics:\")\n",
    "    for key, value in final_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Fine-tuning completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

