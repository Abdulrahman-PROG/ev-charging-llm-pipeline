{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-1",
   "metadata": {},
   "source": [
    "# EV Charging Data Collection - Complete Pipeline\n",
    "This notebook processes PDF files and web scraping to extract text data for training an LLM on electric vehicle charging domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e483848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder created: output_data\n"
     ]
    }
   ],
   "source": [
    "# Create output folder for all generated files\n",
    "output_folder = \"output_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Output folder created: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-pdf",
   "metadata": {},
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e34d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa76452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(pdf_folder):\n",
    "    \"\"\"Process all PDFs in a folder\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            print(f\"Processing PDF: {filename}\")\n",
    "            \n",
    "            text = extract_pdf_text(pdf_path)\n",
    "            if text:\n",
    "                data.append({\n",
    "                    'source': 'pdf',\n",
    "                    'filename': filename,\n",
    "                    'text': text,\n",
    "                    'length': len(text),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-web",
   "metadata": {},
   "source": [
    "## Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "web-scraping-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webpage(url):\n",
    "    \"\"\"Scrape text content from a webpage\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text and clean it\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "web-scraping-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_content(text, url):\n",
    "    \"\"\"Check if content is relevant to EV charging\"\"\"\n",
    "    ev_keywords = [\n",
    "        'charging station', 'electric vehicle', 'ev charging', 'chargepoint',\n",
    "        'supercharger', 'fast charging', 'charging network', 'charging infrastructure'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    # Check for keywords in text\n",
    "    keyword_count = sum(1 for keyword in ev_keywords if keyword in text_lower)\n",
    "    \n",
    "    # Check for keywords in URL\n",
    "    url_keyword_count = sum(1 for keyword in ev_keywords if keyword.replace(' ', '') in url_lower)\n",
    "    \n",
    "    # Content is relevant if it has keywords or URL indicates relevance\n",
    "    return keyword_count >= 1 or url_keyword_count >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "web-scraping-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_websites(urls):\n",
    "    \"\"\"Scrape multiple websites for EV charging content\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Scraping: {url}\")\n",
    "        text = scrape_webpage(url)\n",
    "        \n",
    "        if text and len(text) > 200 and is_relevant_content(text, url):\n",
    "            data.append({\n",
    "                'source': 'web',\n",
    "                'url': url,\n",
    "                'text': text,\n",
    "                'length': len(text),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            print(f\"  ✓ Collected {len(text):,} characters\")\n",
    "        else:\n",
    "            print(f\"  ✗ Skipped (not relevant or too short)\")\n",
    "        \n",
    "        # Be respectful to websites\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-collection",
   "metadata": {},
   "source": [
    "## Data Collection Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06903a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing PDF Files ===\n",
      "Processing PDF: transatlantic technical recommendations for government-KJ0423560ENN.pdf\n",
      "Processing PDF: optimal allocation of electric vehicle charging infrastructure-LDNA27894ENN.pdf\n",
      "Processing PDF: electric vehicle charging concessions-QH0122144ENN.pdf\n",
      "Processing PDF: guidance of fire safety for electric vehicles parked-MI0125004ENN.pdf\n",
      "\n",
      "=== Web Scraping ===\n",
      "Scraping: https://afdc.energy.gov/fuels/electricity_charging_home.html\n",
      "  ✓ Collected 5,342 characters\n",
      "Scraping: https://afdc.energy.gov/fuels/electricity_charging_public.html\n",
      "  ✓ Collected 4,656 characters\n",
      "Scraping: https://www.energy.gov/eere/electricvehicles/charging-home\n",
      "Error scraping https://www.energy.gov/eere/electricvehicles/charging-home: 403 Client Error: Forbidden for url: https://www.energy.gov/eere/electricvehicles/charging-home\n",
      "  ✗ Skipped (not relevant or too short)\n",
      "Scraping: https://www.energy.gov/eere/electricvehicles/electric-vehicle-charging-infrastructure\n",
      "Error scraping https://www.energy.gov/eere/electricvehicles/electric-vehicle-charging-infrastructure: 404 Client Error: Not Found for url: https://www.energy.gov/eere/electricvehicles/electric-vehicle-charging-infrastructure\n",
      "  ✗ Skipped (not relevant or too short)\n",
      "\n",
      "=== Collection Summary ===\n",
      "PDF files processed: 4\n",
      "Web pages scraped: 2\n",
      "Total data sources: 6\n",
      "- PDF: transatlantic technical recommendations for government-KJ0423560ENN.pdf: 27,354 characters\n",
      "- PDF: optimal allocation of electric vehicle charging infrastructure-LDNA27894ENN.pdf: 53,302 characters\n",
      "- PDF: electric vehicle charging concessions-QH0122144ENN.pdf: 90,913 characters\n",
      "- PDF: guidance of fire safety for electric vehicles parked-MI0125004ENN.pdf: 180,062 characters\n",
      "- Web: https://afdc.energy.gov/fuels/electricity_charging_home.html: 5,342 characters\n",
      "- Web: https://afdc.energy.gov/fuels/electricity_charging_public.html: 4,656 characters\n"
     ]
    }
   ],
   "source": [
    "# Define URLs for web scraping\n",
    "ev_urls = [\n",
    "    \"https://afdc.energy.gov/fuels/electricity_charging_home.html\",\n",
    "    \"https://afdc.energy.gov/fuels/electricity_charging_public.html\",\n",
    "    \"https://www.energy.gov/eere/electricvehicles/charging-home\",\n",
    "    \"https://www.energy.gov/eere/electricvehicles/electric-vehicle-charging-infrastructure\"\n",
    "]\n",
    "\n",
    "# Collect data from PDFs\n",
    "print(\"=== Processing PDF Files ===\")\n",
    "pdf_folder = \"pdfs\"\n",
    "pdf_data = process_pdfs(pdf_folder)\n",
    "\n",
    "# Collect data from websites\n",
    "print(\"\\n=== Web Scraping ===\")\n",
    "web_data = scrape_websites(ev_urls)\n",
    "\n",
    "# Combine all data\n",
    "all_data = pdf_data + web_data\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n=== Collection Summary ===\")\n",
    "print(f\"PDF files processed: {len(pdf_data)}\")\n",
    "print(f\"Web pages scraped: {len(web_data)}\")\n",
    "print(f\"Total data sources: {len(all_data)}\")\n",
    "\n",
    "for item in all_data:\n",
    "    if item['source'] == 'pdf':\n",
    "        print(f\"- PDF: {item['filename']}: {item['length']:,} characters\")\n",
    "    else:\n",
    "        print(f\"- Web: {item['url']}: {item['length']:,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4e3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data saved to output_data/ev_raw_data.csv\n",
      "\n",
      "Data summary:\n",
      "       length                 \n",
      "        count     sum     mean\n",
      "source                        \n",
      "pdf         4  351631  87908.0\n",
      "web         2    9998   4999.0\n"
     ]
    }
   ],
   "source": [
    "# Save raw data to CSV in output folder\n",
    "df = pd.DataFrame(all_data)\n",
    "csv_path = os.path.join(output_folder, 'ev_raw_data.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Raw data saved to {csv_path}\")\n",
    "\n",
    "# Display data summary\n",
    "print(\"\\nData summary:\")\n",
    "print(df.groupby('source').agg({\n",
    "    'length': ['count', 'sum', 'mean']\n",
    "}).round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-2",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "Create Q&A pairs from the extracted text for LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "training-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=500):\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk + sentence) < chunk_size:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "training-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(text_chunks, source_info):\n",
    "    \"\"\"Generate simple Q&A pairs from text chunks\"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        if len(chunk) > 100:  # Only use substantial chunks\n",
    "            # Simple question generation based on content\n",
    "            if \"charging station\" in chunk.lower():\n",
    "                question = \"What information is available about charging stations?\"\n",
    "                qa_pairs.append({\n",
    "                    \"instruction\": question,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": chunk,\n",
    "                    \"source\": source_info\n",
    "                })\n",
    "            \n",
    "            if \"electric vehicle\" in chunk.lower():\n",
    "                question = \"Tell me about electric vehicles and charging.\"\n",
    "                qa_pairs.append({\n",
    "                    \"instruction\": question,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": chunk,\n",
    "                    \"source\": source_info\n",
    "                })\n",
    "            \n",
    "            if \"fast charging\" in chunk.lower() or \"dc charging\" in chunk.lower():\n",
    "                question = \"How does fast charging work for electric vehicles?\"\n",
    "                qa_pairs.append({\n",
    "                    \"instruction\": question,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": chunk,\n",
    "                    \"source\": source_info\n",
    "                })\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "training-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating Training Data ===\n",
      "Processing: transatlantic technical recommendations for government-KJ0423560ENN.pdf\n",
      "  Generated 8 Q&A pairs\n",
      "Processing: optimal allocation of electric vehicle charging infrastructure-LDNA27894ENN.pdf\n",
      "  Generated 47 Q&A pairs\n",
      "Processing: electric vehicle charging concessions-QH0122144ENN.pdf\n",
      "  Generated 20 Q&A pairs\n",
      "Processing: guidance of fire safety for electric vehicles parked-MI0125004ENN.pdf\n",
      "  Generated 95 Q&A pairs\n",
      "Processing: https://afdc.energy.gov/fuels/electricity_charging_home.html\n",
      "  Generated 6 Q&A pairs\n",
      "Processing: https://afdc.energy.gov/fuels/electricity_charging_public.html\n",
      "  Generated 16 Q&A pairs\n",
      "\n",
      "Total training examples: 192\n"
     ]
    }
   ],
   "source": [
    "# Generate training data from all sources\n",
    "print(\"=== Generating Training Data ===\")\n",
    "all_qa_pairs = []\n",
    "\n",
    "for item in all_data:\n",
    "    source_info = item['filename'] if item['source'] == 'pdf' else item['url']\n",
    "    print(f\"Processing: {source_info}\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = split_text(item['text'])\n",
    "    \n",
    "    # Generate Q&A pairs\n",
    "    qa_pairs = generate_qa_pairs(chunks, source_info)\n",
    "    all_qa_pairs.extend(qa_pairs)\n",
    "    \n",
    "    print(f\"  Generated {len(qa_pairs)} Q&A pairs\")\n",
    "\n",
    "print(f\"\\nTotal training examples: {len(all_qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "training-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 192 training examples to output_data/ev_training_data.json\n",
      "Saved Alpaca format training data to output_data/ev_training_alpaca.json\n",
      "\n",
      "First example:\n",
      "Question: Tell me about electric vehicles and charging.\n",
      "Answer: May 2023\n",
      "Transatlantic Technical Recommendations\n",
      "for Government Funded Implementation of\n",
      "Electric Vehicle Charging Infrastructure\n",
      "EU-U. S.  Trade and Technology Council\n",
      "Working Group 2 - Climate and C...\n",
      "Source: transatlantic technical recommendations for government-KJ0423560ENN.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save training data to JSON in output folder\n",
    "training_path = os.path.join(output_folder, 'ev_training_data.json')\n",
    "with open(training_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(all_qa_pairs)} training examples to {training_path}\")\n",
    "\n",
    "# Save training data in Alpaca format (without source info for training)\n",
    "alpaca_data = [{\n",
    "    \"instruction\": item[\"instruction\"],\n",
    "    \"input\": item[\"input\"],\n",
    "    \"output\": item[\"output\"]\n",
    "} for item in all_qa_pairs]\n",
    "\n",
    "alpaca_path = os.path.join(output_folder, 'ev_training_alpaca.json')\n",
    "with open(alpaca_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved Alpaca format training data to {alpaca_path}\")\n",
    "\n",
    "# Show first example\n",
    "if all_qa_pairs:\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(f\"Question: {all_qa_pairs[0]['instruction']}\")\n",
    "    print(f\"Answer: {all_qa_pairs[0]['output'][:200]}...\")\n",
    "    print(f\"Source: {all_qa_pairs[0]['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "All output files are saved in the `output_data` folder:\n",
    "- `ev_raw_data.csv`: Raw extracted data from PDFs and web scraping\n",
    "- `ev_training_data.json`: Training data with source information\n",
    "- `ev_training_alpaca.json`: Training data in Alpaca format for model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev_charging_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
