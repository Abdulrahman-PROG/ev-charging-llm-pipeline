{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-1",
   "metadata": {},
   "source": [
    "# EV Charging LLM Evaluation and Benchmarking\n",
    "This notebook evaluates the fine-tuned model using domain-specific benchmarks and automated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"base_model_name\": \"microsoft/DialoGPT-small\",\n",
    "    \"fine_tuned_model_path\": \"./fine_tuned_model/final_model\",\n",
    "    \"test_data_path\": \"output_data/ev_training_alpaca.json\",\n",
    "    \"benchmark_size\": 50,  # Number of test examples\n",
    "    \"max_length\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "    \"results_dir\": \"evaluation_results\"\n",
    "}\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Evaluation configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize sentence transformer for semantic similarity\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Sentence transformer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-data",
   "metadata": {},
   "source": [
    "## Load Models and Create Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model_name\"])\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model_name\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "print(\"Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-fine-tuned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "try:\n",
    "    ft_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"fine_tuned_model_path\"])\n",
    "    ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"fine_tuned_model_path\"],\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    \n",
    "    if ft_tokenizer.pad_token is None:\n",
    "        ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
    "    \n",
    "    print(\"Fine-tuned model loaded successfully\")\n",
    "    model_loaded = True\n",
    "except Exception as e:\n",
    "    print(f\"Could not load fine-tuned model: {e}\")\n",
    "    print(\"Will use base model for comparison\")\n",
    "    ft_model = base_model\n",
    "    ft_tokenizer = base_tokenizer\n",
    "    model_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain-specific benchmark dataset\n",
    "def create_ev_benchmark():\n",
    "    \"\"\"Create EV charging domain benchmark questions\"\"\"\n",
    "    benchmark_questions = [\n",
    "        {\n",
    "            \"question\": \"What are the different types of EV charging connectors?\",\n",
    "            \"expected_keywords\": [\"Type 1\", \"Type 2\", \"CHAdeMO\", \"CCS\", \"connector\", \"charging\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How long does it take to charge an electric vehicle?\",\n",
    "            \"expected_keywords\": [\"time\", \"hours\", \"fast\", \"slow\", \"charging\", \"battery\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the difference between AC and DC charging?\",\n",
    "            \"expected_keywords\": [\"AC\", \"DC\", \"alternating\", \"direct\", \"current\", \"charging\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Where can I find public charging stations?\",\n",
    "            \"expected_keywords\": [\"public\", \"stations\", \"location\", \"map\", \"app\", \"network\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is fast charging for electric vehicles?\",\n",
    "            \"expected_keywords\": [\"fast\", \"rapid\", \"DC\", \"charging\", \"quick\", \"speed\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How much does it cost to charge an electric vehicle?\",\n",
    "            \"expected_keywords\": [\"cost\", \"price\", \"money\", \"charging\", \"electricity\", \"rate\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can I charge my EV at home?\",\n",
    "            \"expected_keywords\": [\"home\", \"residential\", \"charging\", \"installation\", \"outlet\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is charging infrastructure?\",\n",
    "            \"expected_keywords\": [\"infrastructure\", \"network\", \"stations\", \"grid\", \"charging\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I install a home charging station?\",\n",
    "            \"expected_keywords\": [\"install\", \"home\", \"electrician\", \"charging\", \"station\", \"setup\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the benefits of electric vehicle charging?\",\n",
    "            \"expected_keywords\": [\"benefits\", \"advantages\", \"clean\", \"environment\", \"cost\", \"charging\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return benchmark_questions\n",
    "\n",
    "# Load test data from training set (last 20% as held-out test)\n",
    "with open(CONFIG[\"test_data_path\"], 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# Use last portion as test set\n",
    "test_size = min(CONFIG[\"benchmark_size\"], len(all_data) // 5)\n",
    "test_data = all_data[-test_size:]\n",
    "\n",
    "# Create domain benchmark\n",
    "domain_benchmark = create_ev_benchmark()\n",
    "\n",
    "print(f\"Created benchmark with {len(domain_benchmark)} domain-specific questions\")\n",
    "print(f\"Using {len(test_data)} examples from training data as test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-generation",
   "metadata": {},
   "source": [
    "## Text Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=200):\n",
    "    \"\"\"Generate response using the given model\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=min(max_length, inputs.shape[1] + 100),\n",
    "            num_return_sequences=1,\n",
    "            temperature=CONFIG[\"temperature\"],\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from response\n",
    "    if prompt in response:\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def format_prompt(question):\n",
    "    \"\"\"Format question as prompt\"\"\"\n",
    "    return f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
    "\n",
    "print(\"Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-metrics",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(reference, hypothesis):\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    reference_tokens = reference.split()\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    score = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def calculate_semantic_similarity(text1, text2):\n",
    "    \"\"\"Calculate semantic similarity using sentence transformers\"\"\"\n",
    "    embeddings = sentence_model.encode([text1, text2])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def keyword_coverage(text, keywords):\n",
    "    \"\"\"Calculate how many expected keywords are covered\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    covered = sum(1 for keyword in keywords if keyword.lower() in text_lower)\n",
    "    return covered / len(keywords) if keywords else 0\n",
    "\n",
    "print(\"Evaluation metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-evaluation",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on domain-specific benchmark\n",
    "print(\"Evaluating on domain-specific benchmark...\")\n",
    "\n",
    "domain_results = []\n",
    "\n",
    "for i, item in enumerate(domain_benchmark):\n",
    "    question = item[\"question\"]\n",
    "    expected_keywords = item[\"expected_keywords\"]\n",
    "    \n",
    "    prompt = format_prompt(question)\n",
    "    \n",
    "    # Generate responses\n",
    "    base_response = generate_response(base_model, base_tokenizer, prompt)\n",
    "    ft_response = generate_response(ft_model, ft_tokenizer, prompt)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    base_coverage = keyword_coverage(base_response, expected_keywords)\n",
    "    ft_coverage = keyword_coverage(ft_response, expected_keywords)\n",
    "    \n",
    "    semantic_sim = calculate_semantic_similarity(base_response, ft_response)\n",
    "    \n",
    "    result = {\n",
    "        'question_id': i + 1,\n",
    "        'question': question,\n",
    "        'base_response': base_response,\n",
    "        'ft_response': ft_response,\n",
    "        'base_keyword_coverage': base_coverage,\n",
    "        'ft_keyword_coverage': ft_coverage,\n",
    "        'semantic_similarity': semantic_sim,\n",
    "        'expected_keywords': expected_keywords\n",
    "    }\n",
    "    \n",
    "    domain_results.append(result)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(domain_benchmark)} questions\")\n",
    "\n",
    "print(f\"Domain benchmark evaluation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-test-set",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "print(\"Evaluating on held-out test set...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i, item in enumerate(test_data[:20]):  # Limit to 20 for demo\n",
    "    instruction = item[\"instruction\"]\n",
    "    reference = item[\"output\"]\n",
    "    \n",
    "    prompt = format_prompt(instruction)\n",
    "    \n",
    "    # Generate responses\n",
    "    base_response = generate_response(base_model, base_tokenizer, prompt)\n",
    "    ft_response = generate_response(ft_model, ft_tokenizer, prompt)\n",
    "    \n",
    "    # Calculate metrics against reference\n",
    "    base_rouge = calculate_rouge_scores(reference, base_response)\n",
    "    ft_rouge = calculate_rouge_scores(reference, ft_response)\n",
    "    \n",
    "    base_bleu = calculate_bleu_score(reference, base_response)\n",
    "    ft_bleu = calculate_bleu_score(reference, ft_response)\n",
    "    \n",
    "    base_semantic = calculate_semantic_similarity(reference, base_response)\n",
    "    ft_semantic = calculate_semantic_similarity(reference, ft_response)\n",
    "    \n",
    "    result = {\n",
    "        'test_id': i + 1,\n",
    "        'instruction': instruction,\n",
    "        'reference': reference,\n",
    "        'base_response': base_response,\n",
    "        'ft_response': ft_response,\n",
    "        'base_rouge1': base_rouge['rouge1'],\n",
    "        'base_rouge2': base_rouge['rouge2'],\n",
    "        'base_rougeL': base_rouge['rougeL'],\n",
    "        'ft_rouge1': ft_rouge['rouge1'],\n",
    "        'ft_rouge2': ft_rouge['rouge2'],\n",
    "        'ft_rougeL': ft_rouge['rougeL'],\n",
    "        'base_bleu': base_bleu,\n",
    "        'ft_bleu': ft_bleu,\n",
    "        'base_semantic': base_semantic,\n",
    "        'ft_semantic': ft_semantic\n",
    "    }\n",
    "    \n",
    "    test_results.append(result)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Processed {i + 1}/20 test examples\")\n",
    "\n",
    "print(f\"Test set evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-analysis",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze domain benchmark results\n",
    "domain_df = pd.DataFrame(domain_results)\n",
    "\n",
    "print(\"=== Domain Benchmark Results ===\")\n",
    "print(f\"Average Base Model Keyword Coverage: {domain_df['base_keyword_coverage'].mean():.3f}\")\n",
    "print(f\"Average Fine-tuned Model Keyword Coverage: {domain_df['ft_keyword_coverage'].mean():.3f}\")\n",
    "print(f\"Average Semantic Similarity: {domain_df['semantic_similarity'].mean():.3f}\")\n",
    "\n",
    "# Improvement in keyword coverage\n",
    "coverage_improvement = domain_df['ft_keyword_coverage'].mean() - domain_df['base_keyword_coverage'].mean()\n",
    "print(f\"Keyword Coverage Improvement: {coverage_improvement:.3f}\")\n",
    "\n",
    "# Analyze test set results\n",
    "test_df = pd.DataFrame(test_results)\n",
    "\n",
    "print(\"\\n=== Test Set Results ===\")\n",
    "print(\"Base Model Metrics:\")\n",
    "print(f\"  ROUGE-1: {test_df['base_rouge1'].mean():.3f}\")\n",
    "print(f\"  ROUGE-2: {test_df['base_rouge2'].mean():.3f}\")\n",
    "print(f\"  ROUGE-L: {test_df['base_rougeL'].mean():.3f}\")\n",
    "print(f\"  BLEU: {test_df['base_bleu'].mean():.3f}\")\n",
    "print(f\"  Semantic Similarity: {test_df['base_semantic'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "print(f\"  ROUGE-1: {test_df['ft_rouge1'].mean():.3f}\")\n",
    "print(f\"  ROUGE-2: {test_df['ft_rouge2'].mean():.3f}\")\n",
    "print(f\"  ROUGE-L: {test_df['ft_rougeL'].mean():.3f}\")\n",
    "print(f\"  BLEU: {test_df['ft_bleu'].mean():.3f}\")\n",
    "print(f\"  Semantic Similarity: {test_df['ft_semantic'].mean():.3f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\nImprovements:\")\n",
    "print(f\"  ROUGE-1: {test_df['ft_rouge1'].mean() - test_df['base_rouge1'].mean():.3f}\")\n",
    "print(f\"  ROUGE-2: {test_df['ft_rouge2'].mean() - test_df['base_rouge2'].mean():.3f}\")\n",
    "print(f\"  ROUGE-L: {test_df['ft_rougeL'].mean() - test_df['base_rougeL'].mean():.3f}\")\n",
    "print(f\"  BLEU: {test_df['ft_bleu'].mean() - test_df['base_bleu'].mean():.3f}\")\n",
    "print(f\"  Semantic Similarity: {test_df['ft_semantic'].mean() - test_df['base_semantic'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Keyword Coverage Comparison\n",
    "coverage_data = {\n",
    "    'Base Model': domain_df['base_keyword_coverage'].tolist(),\n",
    "    'Fine-tuned Model': domain_df['ft_keyword_coverage'].tolist()\n",
    "}\n",
    "coverage_df = pd.DataFrame(coverage_data)\n",
    "coverage_df.boxplot(ax=axes[0,0])\n",
    "axes[0,0].set_title('Keyword Coverage Comparison')\n",
    "axes[0,0].set_ylabel('Coverage Score')\n",
    "\n",
    "# 2. ROUGE Scores Comparison\n",
    "rouge_metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "base_rouge = [test_df['base_rouge1'].mean(), test_df['base_rouge2'].mean(), test_df['base_rougeL'].mean()]\n",
    "ft_rouge = [test_df['ft_rouge1'].mean(), test_df['ft_rouge2'].mean(), test_df['ft_rougeL'].mean()]\n",
    "\n",
    "x = np.arange(len(rouge_metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,1].bar(x - width/2, base_rouge, width, label='Base Model', alpha=0.8)\n",
    "axes[0,1].bar(x + width/2, ft_rouge, width, label='Fine-tuned Model', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Metrics')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_title('ROUGE Scores Comparison')\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(rouge_metrics)\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. BLEU and Semantic Similarity\n",
    "metrics = ['BLEU', 'Semantic Similarity']\n",
    "base_scores = [test_df['base_bleu'].mean(), test_df['base_semantic'].mean()]\n",
    "ft_scores = [test_df['ft_bleu'].mean(), test_df['ft_semantic'].mean()]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "axes[1,0].bar(x - width/2, base_scores, width, label='Base Model', alpha=0.8)\n",
    "axes[1,0].bar(x + width/2, ft_scores, width, label='Fine-tuned Model', alpha=0.8)\n",
    "axes[1,0].set_xlabel('Metrics')\n",
    "axes[1,0].set_ylabel('Score')\n",
    "axes[1,0].set_title('BLEU and Semantic Similarity')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(metrics)\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Improvement Summary\n",
    "improvements = {\n",
    "    'Keyword Coverage': coverage_improvement,\n",
    "    'ROUGE-1': test_df['ft_rouge1'].mean() - test_df['base_rouge1'].mean(),\n",
    "    'ROUGE-L': test_df['ft_rougeL'].mean() - test_df['base_rougeL'].mean(),\n",
    "    'BLEU': test_df['ft_bleu'].mean() - test_df['base_bleu'].mean(),\n",
    "    'Semantic Sim': test_df['ft_semantic'].mean() - test_df['base_semantic'].mean()\n",
    "}\n",
    "\n",
    "metrics = list(improvements.keys())\n",
    "values = list(improvements.values())\n",
    "colors = ['green' if v > 0 else 'red' for v in values]\n",
    "\n",
    "axes[1,1].bar(metrics, values, color=colors, alpha=0.7)\n",
    "axes[1,1].set_title('Performance Improvements')\n",
    "axes[1,1].set_ylabel('Improvement Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"results_dir\"], 'evaluation_results.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization saved to {CONFIG['results_dir']}/evaluation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-examples",
   "metadata": {},
   "source": [
    "## Example Responses Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example responses\n",
    "print(\"=== Example Response Comparisons ===\")\n",
    "\n",
    "for i in range(min(3, len(domain_results))):\n",
    "    result = domain_results[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Expected Keywords: {', '.join(result['expected_keywords'])}\")\n",
    "    print(f\"\\nBase Model Response:\")\n",
    "    print(f\"{result['base_response'][:200]}...\")\n",
    "    print(f\"Keyword Coverage: {result['base_keyword_coverage']:.2f}\")\n",
    "    print(f\"\\nFine-tuned Model Response:\")\n",
    "    print(f\"{result['ft_response'][:200]}...\")\n",
    "    print(f\"Keyword Coverage: {result['ft_keyword_coverage']:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-save",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "domain_df.to_csv(os.path.join(CONFIG[\"results_dir\"], 'domain_benchmark_results.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(CONFIG[\"results_dir\"], 'test_set_results.csv'), index=False)\n",
    "\n",
    "# Save summary metrics\n",
    "summary = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'base_model': CONFIG['base_model_name'],\n",
    "    'fine_tuned_model': CONFIG['fine_tuned_model_path'],\n",
    "    'domain_benchmark_size': len(domain_results),\n",
    "    'test_set_size': len(test_results),\n",
    "    'metrics': {\n",
    "        'keyword_coverage_improvement': coverage_improvement,\n",
    "        'rouge1_improvement': test_df['ft_rouge1'].mean() - test_df['base_rouge1'].mean(),\n",
    "        'rouge2_improvement': test_df['ft_rouge2'].mean() - test_df['base_rouge2'].mean(),\n",
    "        'rougeL_improvement': test_df['ft_rougeL'].mean() - test_df['base_rougeL'].mean(),\n",
    "        'bleu_improvement': test_df['ft_bleu'].mean() - test_df['base_bleu'].mean(),\n",
    "        'semantic_similarity_improvement': test_df['ft_semantic'].mean() - test_df['base_semantic'].mean()\n",
    "    },\n",
    "    'base_model_performance': {\n",
    "        'avg_keyword_coverage': domain_df['base_keyword_coverage'].mean(),\n",
    "        'avg_rouge1': test_df['base_rouge1'].mean(),\n",
    "        'avg_rouge2': test_df['base_rouge2'].mean(),\n",
    "        'avg_rougeL': test_df['base_rougeL'].mean(),\n",
    "        'avg_bleu': test_df['base_bleu'].mean(),\n",
    "        'avg_semantic': test_df['base_semantic'].mean()\n",
    "    },\n",
    "    'fine_tuned_model_performance': {\n",
    "        'avg_keyword_coverage': domain_df['ft_keyword_coverage'].mean(),\n",
    "        'avg_rouge1': test_df['ft_rouge1'].mean(),\n",
    "        'avg_rouge2': test_df['ft_rouge2'].mean(),\n",
    "        'avg_rougeL': test_df['ft_rougeL'].mean(),\n",
    "        'avg_bleu': test_df['ft_bleu'].mean(),\n",
    "        'avg_semantic': test_df['ft_semantic'].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], 'evaluation_summary.json'), 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {CONFIG['results_dir']}/\")\n",
    "print(\"Files created:\")\n",
    "print(\"- domain_benchmark_results.csv\")\n",
    "print(\"- test_set_results.csv\")\n",
    "print(\"- evaluation_summary.json\")\n",
    "print(\"- evaluation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-conclusion",
   "metadata": {},
   "source": [
    "## Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"           EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä EVALUATION OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Base Model: {CONFIG['base_model_name']}\")\n",
    "print(f\"   ‚Ä¢ Fine-tuned Model: {CONFIG['fine_tuned_model_path']}\")\n",
    "print(f\"   ‚Ä¢ Domain Benchmark: {len(domain_results)} questions\")\n",
    "print(f\"   ‚Ä¢ Test Set: {len(test_results)} examples\")\n",
    "\n",
    "print(f\"\\nüéØ KEY IMPROVEMENTS:\")\n",
    "print(f\"   ‚Ä¢ Keyword Coverage: {coverage_improvement:+.3f}\")\n",
    "print(f\"   ‚Ä¢ ROUGE-1: {test_df['ft_rouge1'].mean() - test_df['base_rouge1'].mean():+.3f}\")\n",
    "print(f\"   ‚Ä¢ ROUGE-L: {test_df['ft_rougeL'].mean() - test_df['base_rougeL'].mean():+.3f}\")\n",
    "print(f\"   ‚Ä¢ BLEU Score: {test_df['ft_bleu'].mean() - test_df['base_bleu'].mean():+.3f}\")\n",
    "print(f\"   ‚Ä¢ Semantic Similarity: {test_df['ft_semantic'].mean() - test_df['base_semantic'].mean():+.3f}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE ANALYSIS:\")\n",
    "if coverage_improvement > 0:\n",
    "    print(f\"   ‚úÖ Fine-tuning improved domain-specific keyword coverage\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è  Fine-tuning did not improve keyword coverage\")\n",
    "\n",
    "rouge_improvement = test_df['ft_rouge1'].mean() - test_df['base_rouge1'].mean()\n",
    "if rouge_improvement > 0:\n",
    "    print(f\"   ‚úÖ Fine-tuning improved ROUGE scores\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è  Fine-tuning did not improve ROUGE scores\")\n",
    "\n",
    "semantic_improvement = test_df['ft_semantic'].mean() - test_df['base_semantic'].mean()\n",
    "if semantic_improvement > 0:\n",
    "    print(f\"   ‚úÖ Fine-tuning improved semantic similarity\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è  Fine-tuning did not improve semantic similarity\")\n",
    "\n",
    "print(f\"\\nüíæ RESULTS SAVED TO: {CONFIG['results_dir']}/\")\n",
    "print(\"\\n‚úÖ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

